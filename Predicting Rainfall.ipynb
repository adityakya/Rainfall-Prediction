{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOW MUCH DID IT RAIN ?\n",
    "\n",
    "For agriculture, it is extremely important to know how much it rained on a particular field. However, rainfall is variable in space and time and it is impossible to have rain gauges everywhere. Therefore, remote sensing instruments such as radar are used to provide wide spatial coverage.\n",
    "\n",
    "Rainfall estimates drawn from remotely sensed observations will never exactly match the measurements that are carried out using rain gauges, due to the inherent characteristics of both sensors. Currently, radar observations are \"corrected\" using nearby gauges and a single estimate of rainfall is provided to users who need to know how much it rained.\n",
    "\n",
    "The Challenge is to solve this in probabilistic manner.Knowing the full probabilistic spread of rainfall amounts can be very useful to drive hydrological and agronomic models -- much more than a single estimate of rainfall.\n",
    "\n",
    "<img src=\"dual_pol2.jpg\">\n",
    "\n",
    "\n",
    "Unlike a conventional Doppler radar, a polarimetric radar transmits radio wave pulses that have both horizontal and vertical orientations. Because rain drops become flatter as they increase in size and because ice crystals tend to be elongated vertically, whereas liquid droplets tend to be flattened, it is possible to infer the size of rain drops and the type of hydrometeor from the differential reflectivity of the two orientations.\n",
    "\n",
    "We are given polarimetric radar values and derived quantities at a location over the period of one hour. You will need to produce a probabilistic distribution of the hourly rain gauge total. \n",
    "\n",
    "#### ABOUT POLAMETRIC RADAR MEASUREMNTS \n",
    "\n",
    "Polarimetric radar offers the promise of being able to better infer drop-sizes and thus improve rainfall estimates since smaller drops evaporate more and of being able to distinguish between echoes due to bioscatter and echoes due to weather.  The US National Weather Service's weather radar network (called NEXRAD) was recently upgraded to polarimetry, and it is the polarimetric radar data collected after the upgrade that you are provided.\n",
    "\n",
    "\n",
    "This is an kaggle competition and you can download the dataset from the link given below:\n",
    "https://www.kaggle.com/c/how-much-did-it-rain/data\n",
    "\n",
    "\n",
    "## LET'S START OUR JOURNEY\n",
    "Before understanding the data set I will be importing the python libraries that will be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-25d5091aa95e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "# importing the basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# IMPORTING other libraires which will be used\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"train_2013.csv\")\n",
    "test=pd.read_csv(\"test_2014.csv\")\n",
    "\n",
    "print(\"Training Size : (%d,%d)\"%train.shape)\n",
    "print(\"Test Size : (%d,%d)\"%test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNDERSTANDING THE DATA \n",
    "There are 19 provided features, with three of these features being rain\n",
    "rates predicted from three current algorithms. These three past algorithm\n",
    "features, RR1, RR2, and RR3, are respectively, the ‘HCA-based’, ‘Zdrbased’,\n",
    "and ‘Kdp-based’ algorithms. The other 16 features are given as time\n",
    "series numerical data. An example data point could have its ‘TimeToEnd’\n",
    "features ‘58.0 55.0 52.0 49.0 41.0,’ indicating radar information taken at 58,\n",
    "55, . . . , 41 minutes from the end of the hour. For this same row, the features\n",
    "‘Reflectivity’ as ‘0.0, 0.0, 1.2, 4.5, 0.0’ and ‘RR1’ as ‘0.0. 0.0, 2.2, 0.3, 0.0’\n",
    "mean these measurements taken at the time points in the ‘TimeToEnd’ series.\n",
    "The label(expected value) for each row is one float number, the amount in mm of rain\n",
    "collected for that hour\n",
    "\n",
    "\n",
    "\n",
    "### DESCRIPTION OF COLUMNS\n",
    "The columns in the datasets are:\n",
    "\n",
    "    TimeToEnd:  How many minutes before the end of the hour was this radar observation?\n",
    "\n",
    "    DistanceToRadar:  Distance between radar and gauge.  This value is scaled and rounded to prevent reverse engineering gauge location\n",
    "\n",
    "    Composite:  Maximum reflectivity in vertical volume above gauge\n",
    "\n",
    "    HybridScan: Reflectivity in elevation scan closest to ground\n",
    "\n",
    "    HydrometeorType:  One of nine categories in NSSL HCA. See presentation for details.\n",
    "\n",
    "    Kdp:  Differential phase\n",
    "\n",
    "    RR1:  Rain rate from HCA-based algorithm\n",
    "\n",
    "    RR2:  Rain rate from Zdr-based algorithm\n",
    "\n",
    "    RR3:  Rain rate from Kdp-based algorithm\n",
    "\n",
    "    RadarQualityIndex:  A value from 0 (bad data) to 1 (good data)\n",
    "\n",
    "    Reflectivity:  In dBZ\n",
    "\n",
    "    ReflectivityQC:  Quality-controlled reflectivity\n",
    "\n",
    "    RhoHV:  Correlation coefficient\n",
    "\n",
    "    Velocity:  (aliased) Doppler velocity\n",
    "\n",
    "    Zdr:  Differential reflectivity in dB\n",
    "\n",
    "    LogWaterVolume:  How much of radar pixel is filled with water droplets?\n",
    "\n",
    "    MassWeightedMean:  Mean drop size in mm\n",
    "\n",
    "    MassWeightedSD:  Standard deviation of drop size\n",
    "\n",
    "    Expected: the actual amount of rain reported by the rain gauge for that hour.\n",
    "\n",
    "#### Hydrometeor types:\n",
    "\n",
    "    0-no echo\n",
    "\n",
    "    1-moderate rain\n",
    "\n",
    "    2-moderate rain\n",
    "\n",
    "    3-heavy rain\n",
    "\n",
    "    4-rain/hail\n",
    "\n",
    "    5-big drops\n",
    "\n",
    "    6-AP\n",
    "\n",
    "    7-Birds\n",
    "\n",
    "    8-unknown\n",
    "\n",
    "    9-no echo\n",
    "\n",
    "    10-dry snow\n",
    "\n",
    "    11-wet snow\n",
    "\n",
    "    12-ice crystals\n",
    "\n",
    "    13-graupel\n",
    "\n",
    "    14-graupel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train['TimeToEnd'][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=pd.read_csv(\"sampleSubmission.csv\")\n",
    "\n",
    "sample.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "Submissions are predictions of the probabilistic distribution of\n",
    "the hourly rain total. Each row of the submission is a list of values P(y ≤ Y ),\n",
    "for Y integer values 0, 1, 2, . . . 69, and y the rainfall total, in mm and it is obivous that for every y \n",
    "\n",
    "                                 P(y≤k) ≤ P(y ≤ k+1) \n",
    "For instance a perfect prediction\n",
    "for the true label of \"2.5\" would be the row 0, 0, 0, 1, 1, . . . , 1 corresponding\n",
    "to P r(y ≤ 0) = P r(y ≤ 1) = P r(y ≤ 2) = 0 and P r(y ≤ 3) = · · · =\n",
    "P r(y ≤ 69) = 1.                                        \n",
    "                                        \n",
    "## MISSING DATA\n",
    "\n",
    " There are five types of missing data\n",
    "\n",
    "-99900: echo below signal-to-noise threshold of radar.  In other words, the true value could be anywhere between -14 and -inf, but we don't know \n",
    "\n",
    "-99901: range folded data\n",
    "\n",
    "-99903: data not collected such as due to beam blockage or beyond data range\n",
    "\n",
    "nan: derived quantity could not be computed because some input was one of the above codes\n",
    "\n",
    "999.0: RadarQualityIndex could not be computed because pixel was at edge of echo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency plot of first 100 expected values. So that we can visualize how are expected values distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(20,20))\n",
    "sns.distplot(train['Expected'].head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BASIC MODELS\n",
    "\n",
    "These are the models which doesn't contain any feature engineering. They are just the simple Statistics and some common sense. \n",
    "\n",
    "Clearly they are approximation but they provide a benchmark to other models which contain some features.\n",
    "\n",
    "There are 987398(87.6367 % of train data) points which have 0 as actual amount of rainfall. That means actually there was no rainfall.\n",
    "\n",
    "So what i will do for test data also that probability of rainfall less than any value is 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans=pd.DataFrame(columns=sample.columns)\n",
    "ans['Id']=test['Id']\n",
    "ans.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=list(sample.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols.remove('Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans[cols]=1                  # making each probability as 1.\n",
    "ans.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans.to_csv(\"No_rain.csv\",index=False)   # got an score of private:0.01025920 and public:0.01017651  \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through this I got an score of private:0.01025920 and public:0.01017651 and after seeing in leaderboard this score will land you to 204-210 rank(not bad!)\n",
    "\n",
    "This is another model which also dont contain any feature engineering. In this what I did was I computed the proportions of classes( which I mentioned below) in the train and put these proportions in the train set irrespective of the features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "length=train.shape[0]\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of predicted0 and predicted1 and all these are same\n",
    "for i in cols:\n",
    "    l=len(i)\n",
    "    if(l==10):\n",
    "        k=int(i[l-1])\n",
    "    else:                     # for handling two digits like 11...\n",
    "        a=int(i[9])\n",
    "        b=int(i[10])\n",
    "        k=a*10+b\n",
    "    ans[i]=(train.loc[train['Expected']<=k,'Expected'].value_counts()/(length)).sum()\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans.to_csv(\"only_train_per.csv\",index=False)           \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By this model I got a score of private:0.00978634 and public:0.00971225. This score will give rank of 188 in public and 191 in private leaderboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPLORATION\n",
    "\n",
    "The above one were our basic models. So now it's time to explore the data and try to build some models\n",
    "\n",
    "Before going further we will have to choose how to model the problem. Here I will model by transforming the problem into classification problem.\n",
    "\n",
    "\n",
    "So from the sample submission we can see that for each id we have 69 columns which represent the probabilities. The first column will represent <=0 probability and second column will represent will represent <= 1 probability and so on.\n",
    "\n",
    "So it is similar to classification algorithm which has 70(?) classes and each class represent the value lying between (i,i+1).\n",
    "\n",
    "The classes will be like this :\n",
    "\n",
    "    if expected value is 0 then it is class 0\n",
    "\n",
    "    if expected value is between 0 to 1(inclusive) then it is class 1\n",
    "\n",
    "    if expected value is between 1 to 2(inclusive) then it is class 2\n",
    "\n",
    "    if expected value is between 2 to 3(inclusive) then it is class 3\n",
    "\n",
    "    ......\n",
    "\n",
    "    ......\n",
    "\n",
    "    if expected value is between 68 to 69(inclusive) then it is class 69\n",
    "\n",
    "So at the total we will have 70 classes.\n",
    "\n",
    "Then P(y<=Y) : summation of all probabilities of the classes 0,(0,1),(1,2),(2,3),(3,4) and so on till (Y-1,Y) which represent each column of sample submission\n",
    "\n",
    "Before this I have to decide  what to do with the training examples which have expected values greater than 69 because in the problem we have to predict till 69mm. So we will see how many of the train examples have expected value greater than 69."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train.loc[train['Expected']>69])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are total 5582 train examples which have greater than 69mm. So now we have to decide to keep this values as seperate class or remove these examples as this can be outliers.\n",
    "So I am removing these values from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the examples which have greater than 69mm ranifall\n",
    "train.drop((train.loc[train['Expected']>69]).index,inplace=True)\n",
    "train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the Expected values to classes.\n",
    "train.loc[train['Expected']==0.0,'Expected']=0\n",
    "\n",
    "for i in range(69):              # max value will go to 68\n",
    "    train.loc[(train['Expected']>i) & (train['Expected']<=(i+1)),'Expected']=(i+1)\n",
    "    \n",
    "train['Expected']=train['Expected'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no training example with label 68. This can be easily seen from graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[(train['Expected']==68),'Expected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(15,9))\n",
    "plt.xticks(rotation='90')                   # for rotation of 90 degree\n",
    "sns.countplot(train['Expected'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " RR1:Rain rate from HCA-based algorithm. I will take the mean of the values at all the times recorded and this will be our first feature\n",
    "\n",
    "## RR1\n",
    "\n",
    "I have chosen RR1 first because when we see train data you can see that if RR1 is non-zero then the rainfall is non-zero and if RR1 contains values like -99900 or -99901 or -99902 then also the expected rainfall was zero.\n",
    "\n",
    "So I will use one feature and that is RR1. The results for this submission are written below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k=list(map(float,train['RR1'][6].split()))\n",
    "l=[]                                                    # empty list \n",
    "for i in train.index:\n",
    "    k=list(map(float,train['RR1'][i].split()))\n",
    "    k=[0 if (x==-99900.0 or x==-99901.0 or x==-99903.0) else x for x in k]\n",
    "    mean=sum(k)/len(k)\n",
    "    l.append(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr1=np.array(l)\n",
    "rr1.shape=(train.shape[0],1)\n",
    "print(rr1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(15,9))\n",
    "plt.scatter(rr1[0:500,:],train['Expected'].head(500),color='blue')\n",
    "plt.xlabel(\"RR1\")\n",
    "plt.ylabel(\"Expected Rainfall\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "for i in test.index:\n",
    "    k=list(map(float,test['RR1'][i].split()))\n",
    "    k=[0 if (x==-99900.0 or x==-99901.0 or x==-99903.0) else x for x in k]\n",
    "    mean=sum(k)/len(k)\n",
    "    l.append(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr1_test=np.array(l)\n",
    "rr1_test.shape=(test.shape[0],1)\n",
    "print(rr1_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I removed the code for submission.\n",
    "\n",
    "With the above model I got a public score of 0.00842523 and private score of 0.00842799 which is not bad. This model was better than the benchmark models.\n",
    "With this score the submission rank was 74 in public and 72 in private. So now we are having a rank of less than 100.\n",
    "I also tried with other learning rates\n",
    "\n",
    "0.1 : The results were decent\n",
    "\n",
    "0.3  : The score got worsened and the error was greater when compared with 0.1 learning rate (got a score of 0.13)\n",
    "Basic models were better than this.\n",
    "\n",
    "0.9 : The results were not good.\n",
    "\n",
    "## RR2\n",
    "\n",
    "As the above model performed better than benchmark models. So I included another feature RR2: Rain rate from Zdr-based algorithm in the training matrix. The results got improved but by a small margin.The scores of this model are given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting rr2 into mean values in train data\n",
    "j=[]                                                    # empty list \n",
    "for i in train.index:\n",
    "    k=list(map(float,train['RR2'][i].split()))\n",
    "    k=[0 if (x==-99900.0 or x==-99901.0 or x==-99903.0) else x for x in k]\n",
    "    mean=sum(k)/len(k)\n",
    "    j.append(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr2=np.array(j)\n",
    "rr2.shape=(train.shape[0],1)\n",
    "print(rr2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(15,9))\n",
    "plt.scatter(rr2[0:500,:],train['Expected'].head(500),color='blue')\n",
    "plt.xlabel(\"RR2\")\n",
    "plt.ylabel(\"Expected Rainfall\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting rr2 values into mean values in test data \n",
    "j=[]                                                    # empty list \n",
    "for i in test.index:\n",
    "    k=list(map(float,test['RR2'][i].split()))\n",
    "    k=[0 if (x==-99900.0 or x==-99901.0 or x==-99903.0) else x for x in k]\n",
    "    mean=sum(k)/len(k)\n",
    "    j.append(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr2_test=np.array(j)\n",
    "rr2_test.shape=(test.shape[0],1)\n",
    "print(rr2_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I removed the code for submission.\n",
    "\n",
    "With RR2 as a feature I got a public score of 0.00839472 and private score of 0.00840271. This score will give rank of 70 in private leaderboard.\n",
    "\n",
    "## RR3\n",
    "\n",
    "\n",
    "I included another feature RR3: Rain rate from Kdp-based algorithm in the training matrix. This model performed bad compared to the above model.The scores of this model are given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=[]                                                    # empty list \n",
    "for i in train.index:\n",
    "    k=list(map(float,train['RR3'][i].split()))\n",
    "    k=[0 if (x==-99900.0 or x==-99901.0 or x==-99903.0) else x for x in k]\n",
    "    mean=sum(k)/len(k)\n",
    "    j.append(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr3=np.array(j)\n",
    "rr3.shape=(train.shape[0],1)\n",
    "print(rr3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(15,9))\n",
    "plt.scatter(rr3[0:500,:],train['Expected'].head(500),color='blue')\n",
    "plt.xlabel(\"RR3\")\n",
    "plt.ylabel(\"Expected Rainfall\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=[]                                                    # empty list \n",
    "for i in test.index:\n",
    "    k=list(map(float,test['RR2'][i].split()))\n",
    "    k=[0 if (x==-99900.0 or x==-99901.0 or x==-99903.0) else x for x in k]\n",
    "    mean=sum(k)/len(k)\n",
    "    j.append(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr3_test=np.array(j)\n",
    "rr3_test.shape=(test.shape[0],1)\n",
    "print(rr3_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I removed the code for submission.\n",
    "\n",
    "With RR3 as another feature I got a public score of 0.00840888 and private score of 0.00841514. The model performed bad compared to above model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADDING MORE FEATURES \n",
    "\n",
    "As we can easily see that from RR1 to RR3 the score has not changed much. So now its time to get to another features and explore more.\n",
    "As the error increased when we added a feature RR3. So i am currently removing it as a feature \n",
    " \n",
    "## Radar Quality Index\n",
    "\n",
    "I will add Radar quality index as another feature.It is given that if it is zero then it is bad data and if it is 1 then it is good data. This will be useful in predicting the rainfall because this will classify the dat as bad or good.\n",
    "\n",
    "The missing values are 999.0 and description was that 999.0 means they can't the compute the radar quality index.\n",
    "Suppose if the observation contains more 999.0 then it is of course a bad data. So I will take the mean of all the values and if the observation is 999.0 I will replace it with zero as zero corresponds to bad data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=[]\n",
    "for i in train.index:\n",
    "    k=list(map(float,train['RadarQualityIndex'][i].split()))\n",
    "    k=[0.0 if x==999.0 else x for x in k]\n",
    "    m=sum(k)/float(len(k))\n",
    "    p.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RQi=np.array(p)\n",
    "RQi.shape=(train.shape[0],1)\n",
    "print(RQi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(15,9))\n",
    "plt.scatter(RQi[0:500,:],train['Expected'].head(500),color='blue')\n",
    "plt.xlabel(\"Radar Quality Index\")\n",
    "plt.ylabel(\"Expected Rainfall\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=[]\n",
    "for i in test.index:\n",
    "    k=list(map(float,test['RadarQualityIndex'][i].split()))\n",
    "    k=[0.0 if x==999.0 else x for x in k]\n",
    "    m=sum(k)/float(len(k))\n",
    "    p.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RQi_test=np.array(p)\n",
    "RQi_test.shape=(test.shape[0],1)\n",
    "print(RQi.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I removed the code for submission\n",
    "\n",
    "With this model I got public score of 0.00835402 and private score of 0.00836117 and this will give you a rank of 65 in private learderboard in public leaderboard also.\n",
    "\n",
    "## NUMBER OF RADAR SCANS\n",
    "\n",
    "I am thinking that if the number of radar scans are more then may be the rainfall prediction will be accurate.\n",
    "So I am going to include this feature. I have read a article on this problem and article suggest that there is good correlation between number of scans and rainfall.\n",
    "\n",
    "Number of scans will be equal to number of spaces plus one in TimeToEnd column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['TimeToEnd'][0].count(\" \")+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberofscans=[]\n",
    "for i in train.index:\n",
    "    number=train['TimeToEnd'][i].count(\" \")+1\n",
    "    numberofscans.append(number)\n",
    "\n",
    "numberofscans=np.array(numberofscans).reshape(train.shape[0],1)\n",
    "print(numberofscans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(15,9))\n",
    "plt.scatter(numberofscans,y.reshape(train.shape[0],1))\n",
    "plt.xlabel(\"NUMBER OF RADAR SCANS\")\n",
    "plt.ylabel(\"RAINFALL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberofscans_test=[]\n",
    "for i in test.index:\n",
    "    number=test['TimeToEnd'][i].count(\" \")+1\n",
    "    numberofscans_test.append(number)\n",
    "\n",
    "numberofscans_test=np.array(numberofscans_test).reshape(test.shape[0],1)\n",
    "print(numberofscans_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I removed for the code for submission.\n",
    "\n",
    "With this model I got a public score of 0.00816744 and private score of 0.00816330 and this will give you a rank of 52 in private leaderboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REFLECTIVITY QC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the train data you can see that reflectivity is zero in most cases if we replace -99900 as zero\n",
    "and while the reflectivity is zero the rainfall is also zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=[]\n",
    "for i in train.index:\n",
    "    k=list(map(float,train['ReflectivityQC'][i].split()))\n",
    "    k=[0 if (x==-99900.0 or x==-99901.0 or x==-99903.0) else x for x in k]\n",
    "    m=sum(k)/len(k)\n",
    "    r.append(m)\n",
    "\n",
    "reflectivity=np.array(r).reshape(train.shape[0],1)\n",
    "print(reflectivity.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=[]\n",
    "for i in test.index:\n",
    "    k=list(map(float,test['ReflectivityQC'][i].split()))\n",
    "    k=[0 if (x==-99900.0 or x==-99901.0 or x==-99903.0) else x for x in k]\n",
    "    m=sum(k)/len(k)\n",
    "    r.append(m)\n",
    "    \n",
    "reflectivity_test=np.array(r).reshape(test.shape[0],1)\n",
    "print(reflectivity_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I removed the code for submission.\n",
    "\n",
    "With this model(Xgboost) I got a public score of 0.00787731 and private score of 0.00786504. With this score you will get rank of 32 in leaderboard.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HYBRID SCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HybridScan: Reflectivity in elevation scan closest to ground is also related to reflectivity. So my plan is to include this feature also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=[]\n",
    "for i in train.index:\n",
    "    k=list(map(float,train['HybridScan'][i].split()))\n",
    "    k=[0 if (x==-99900.0 or x==-99901.0 or x==-99903.0) else x for x in k]\n",
    "    m=sum(k)/len(k)\n",
    "    r.append(m)\n",
    "\n",
    "hybrid=np.array(r).reshape(train.shape[0],1)\n",
    "print(hybrid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=[]\n",
    "for i in test.index:\n",
    "    k=list(map(float,test['HybridScan'][i].split()))\n",
    "    k=[0 if (x==-99900.0 or x==-99901.0 or x==-99903.0) else x for x in k]\n",
    "    m=sum(k)/len(k)\n",
    "    r.append(m)\n",
    "    \n",
    "hybrid_test=np.array(r).reshape(test.shape[0],1)\n",
    "print(hybrid_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING MATRICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.hstack((rr1,rr2,RQi,numberofscans,reflectivity,hybrid))\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=np.hstack((rr1_test,rr2_test,RQi_test,numberofscans_test,reflectivity_test,hybrid_test))\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING\n",
    "\n",
    "This time we will be using two models random forest and Xgboost and we will see the individual score and after that I am thinking to ensemble these models with weighted voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb=XGBClassifier()\n",
    "xgb.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgb_predict=xgb.predict_proba(X_test)\n",
    "print(xgb_predict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp=xgb_predict[:,68].reshape(test.shape[0],1)\n",
    "xgb_predict[:,68]=0.0\n",
    "xgb_predict=np.hstack((xgb_predict,temp))\n",
    "print(xgb_predict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predict=np.cumsum(xgb_predict,axis=1)\n",
    "print(xgb_predict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hybrid_data=pd.DataFrame(xgb_predict,columns=cols)\n",
    "hybrid_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_data=pd.concat([test['Id'],hybrid_data],axis=1)\n",
    "hybrid_data.to_csv(\"hybrid.csv\",index=False)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this i got a public score of 0.00783509 and private score of 0.00782772 and a rank of 31 in private leaderboard.\n",
    "Till now this is the best model which outperformed all other models. \n",
    "\n",
    "I also tried ensembling with random forest algorithm but the results didn't improved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANDOM FOREST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=RandomForestClassifier(n_estimators=10,random_state=42)\n",
    "rf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=rf.predict_proba(X_test)\n",
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=f[:,68].reshape(test.shape[0],1)\n",
    "f[:,68]=0\n",
    "f=np.hstack((f,temp))\n",
    "f=np.cumsum(f,axis=1)\n",
    "print(f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hybrid_rf=pd.DataFrame(f,columns=cols)\n",
    "hybrid_rf=pd.concat([test['Id'],hybrid_rf],axis=1)\n",
    "hybrid_rf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_rf[hybrid_rf[cols]>1]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_rf.to_csv(\"hybrid_rf.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENSEMBLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensembling is a general term for combining many classifiers by averaging or voting.Generally, ensembles of classifiers perform better than single classifiers, and the averaging process allows for moregranularity of choice in the bias-variance tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hybrid_data.shape,hybrid_rf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AVERAGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble=hybrid_data[cols]*0.8+hybrid_rf[cols]*0.2\n",
    "ensemble.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble=pd.concat([test['Id'],ensemble],axis=1)\n",
    "ensemble.to_csv(\"random_forest+Xgboost.csv\",index=False)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This end's our journey of predicting the rainfall \n",
    "\n",
    "\n",
    "In Chinese mythology, the one who can predict the rain is known as the messenger who can talk to \"the ruler of the ocean, the dragon king\" ! :) :)\n",
    "\n",
    "Thanks for reading !!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
